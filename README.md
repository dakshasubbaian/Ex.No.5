# **EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS AND EXPLAIN WITH VARIOUS TEST SCENARIOS**
## DATE:
## **Aim**:

To test and compare how different pattern models (naïve/unstructured vs. basic/structured) respond to various prompts across multiple scenarios, and analyze the quality, accuracy, and depth of the generated responses.

---

## **AI Tool Required**:

ChatGPT (or any large language model)

---

## **Explanation**:

### **Prompt Types**:

1. **Naïve Prompt** – A broad, vague, or unstructured request with little detail.
2. **Basic Prompt** – A clear, structured, and detailed instruction that guides the model more precisely.

---

## **Test Scenarios and Prompts**

| **Scenario**                  | **Naïve Prompt**    | **Basic Prompt**                                                                                                                                     | **Naïve Response (Sample)**                               | **Basic Response (Sample)**                                                                                                    | **Analysis**                                                     |
| ----------------------------- | ------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------- |
| **1. Creative Story**         | “Write a story.”    | “Write a short fantasy story about a young girl who discovers a magical key in the forest that leads her to another world. Keep it under 200 words.” | A generic story, lacking depth or theme.                  | A clear, engaging fantasy story with structure and focus.                                                                      | Basic prompt gave more relevant, imaginative, and concise story. |
| **2. Factual Question**       | “Tell me about AI.” | “Explain what Artificial Intelligence is, its types, and give one real-world example in simple words.”                                               | A vague definition, may miss types/examples.              | A well-structured answer explaining definition, types (Narrow, General, Strong AI), with an example (Siri, self-driving cars). | Basic prompt improves clarity and depth.                         |
| **3. Summarization**          | “Summarize.”        | “Summarize the concept of Machine Learning in less than 5 bullet points for beginners.”                                                              | Model may not know what to summarize; gives unclear text. | Clear 5-point summary of ML concepts.                                                                                          | Basic prompt directs model to concise, structured output.        |
| **4. Advice/Recommendation**  | “Give me advice.”   | “Suggest 3 time management tips for college students balancing studies and part-time work.”                                                          | Generic life advice.                                      | Practical and specific time-management strategies (use planner, prioritize tasks, set study-work balance).                     | Basic prompt ensures actionable advice.                          |
| **5. Explanation of Concept** | “Explain cloud.”    | “Explain what cloud computing is, list its advantages, and give one example of a cloud service provider.”                                            | Ambiguous explanation (could mean weather).               | Detailed explanation of cloud computing, with benefits (scalability, cost-effectiveness), and example (AWS, Azure).            | Basic prompt prevents misinterpretation.                         |

---

## **Analysis of Responses**:

* **Quality**: Basic prompts consistently generated more coherent and structured outputs compared to naïve prompts.
* **Accuracy**: Factual accuracy improved when context and details were specified in basic prompts.
* **Depth**: Basic prompts enabled deeper, task-relevant insights while naïve prompts often gave surface-level responses.
* **Consistency**: In almost all scenarios, structured prompts provided better results. Naïve prompts only worked well in very simple, open-ended creative tasks.

---

## **Summary of Findings**:

* **Prompt clarity strongly influences output quality.**
* Naïve prompts often yield vague, generic, or misinterpreted answers.
* Basic prompts produce structured, accurate, and richer responses.
* The more specific and contextual the prompt, the better the AI performs.
* Best practice: Always provide **context + structure + constraints** for optimal results.

---

## **Result**:

Thus, the prompts were executed successfully.

