

# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS AND EXPLAIN WITH VARIOUS TEST SCENARIOS
## DATE:
# Aim:
To test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios.  Analyze the quality, accuracy, and depth of the generated responses 

### AI Tools Required: 
ChatGPT

# Explanation: 
Define the Two Prompt Types:

Write a basic Prompt: Clear, detailed, and structured prompts that give specific instructions or context to guide the model.
Based on that pattern type refined the prompt and submit that with AI tool.
Get the ouput and write the report.

Prepare Multiple Test Scenarios:
Select various scenarios such as:
Generating a creative story.
Answering a factual question.
Summarizing an article or concept.
Providing advice or recommendations.
Or Any other test scenario
For each scenario, create both a naïve and a basic prompt. Ensure each pair of prompts targets the same task but with different levels of structure.
Run Experiments with ChatGPT:
Input the naïve prompt for each scenario and record the generated response.
Then input the corresponding basic prompt and capture that response.
Repeat this process for all selected scenarios to gather a full set of results.
Evaluate Responses : 
	Compare how ChatGPT performs when given naïve versus basic prompts and analyze the output based on Quality,Accuracy and Depth. Also analyse does ChatGPT consistently provide better results with basic prompts? Are there scenarios where naïve prompts work equally well?
Deliverables:
A table comparing ChatGPT's responses to naïve and basic prompts across all scenarios.
Analysis of how prompt clarity impacts the quality, accuracy, and depth of ChatGPT’s outputs.
Summary of findings with insights on how to structure prompts for optimal results when using ChatGPT.


# OUTPUT:
# Test Scenarios & Results:
| Scenario                     | Naïve Prompt                  | ChatGPT Response (Naïve)                                 | Basic Prompt                                                                                                                                                   | ChatGPT Response (Basic)                                                 | Comparison (Quality, Accuracy, Depth)                            |
| ---------------------------- | ----------------------------- | -------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ | ---------------------------------------------------------------- |
| **1. Creative Story**        | *“Tell me a story.”*          | A short generic story without strong characters or plot. | *“Write a 300-word fantasy story about a young girl who discovers a hidden portal in her school leading to a magical world. Include conflict and resolution.”* | A detailed fantasy story with setting, characters, conflict, and ending. | Basic prompt produced richer, more coherent, and engaging story. |
| **2. Factual Question**      | *“Who is Einstein?”*          | General overview, simple facts.                          | *“Explain in 150 words who Albert Einstein was, highlighting his contributions to physics and his theory of relativity in simple terms.”*                      | Concise, structured answer with key contributions, easy to understand.   | Basic prompt improved clarity, relevance, and focus.             |
| **3. Summarization**         | *“Summarize AI.”*             | Vague, 5–6 sentences about AI.                           | *“Summarize Artificial Intelligence in 100 words, covering its definition, key applications, and potential impact on industries.”*                             | Well-structured summary with definition, examples, and impact.           | Basic prompt gave more precise and targeted summary.             |
| **4. Advice/Recommendation** | *“Give me advice.”*           | Very general motivational advice.                        | *“Give me 5 practical tips for staying productive while studying for college exams, with short explanations.”*                                                 | Clear list of 5 actionable study strategies.                             | Basic prompt provided usable, structured advice.                 |
| **5. Concept Explanation**   | *“Explain Machine Learning.”* | General explanation, a bit broad.                        | *“Explain Machine Learning in simple terms for beginners, include types (supervised, unsupervised, reinforcement) and a real-world example.”*                  | Beginner-friendly, structured, examples included.                        | Basic prompt improved depth, clarity, and accessibility.         |

# Analysis:

1.Quality – Responses to naïve prompts were generic, sometimes vague. Basic prompts produced higher quality, structured, and targeted responses.

2.Accuracy – Both prompt types gave correct facts, but basic prompts ensured focus on the required information without unnecessary details.

3.Depth – Naïve prompts gave surface-level answers, while basic prompts encouraged deeper explanations, context, and richer outputs.

Observation: ChatGPT consistently gave better results with basic prompts.

Exception: For very simple tasks (like “Who is Einstein?”), naïve prompts were adequate but still less refined than basic ones.

# Summary of Findings:
1.Prompt clarity matters: Well-structured prompts lead to more accurate, deeper, and useful outputs.

2.Naïve prompts often yield vague, incomplete, or overly generic responses.

3.Basic prompts guide the model, ensuring relevance, completeness, and user-focused results.

For optimal results, prompts should be clear, detailed, and task-specific.


# RESULT: 
The prompt for the above said problem executed successfully
